{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### to load your drive content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShqHJBxVY2YP",
        "outputId": "63726844-7367-4938-cd09-04c746a90655"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7JlxWssyU7F",
        "outputId": "aee827b2-9201-47f1-fe19-59b0ccdf8c41"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Sz0wwaZFzs",
        "outputId": "bdd8d15d-37dc-40d6-f3ca-fc9c03601532"
      },
      "outputs": [],
      "source": [
        "%pip install numpy scikit-learn torch torchvision opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### this is main.py code just for google colab to get the trainand test dataset from drive \n",
        "\n",
        "```\n",
        "also you can run the following code to train the model , and it takes too long so after\n",
        "each Epochs you can stop coz its gonna store the previous step in checkpoint.pth so that you can resume from where you left \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Hp9AibRDYY",
        "outputId": "6ef13b35-4e40-43bc-9e31-feed4001e3d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "\n",
        "# **Configuration for the model**\n",
        "model_name = \"efficientnet_b0\"  # Faster and more accurate than ResNet50\n",
        "num_classes = 25  # Number of bird species\n",
        "batch_size = 256  # Increased for faster training\n",
        "learning_rate = 0.0005  # Slightly reduced for better convergence\n",
        "epochs = 15  # Reduced from 20 to save time\n",
        "max_images_per_class = 400  # **Change to 800 for better accuracy**\n",
        "\n",
        "# **Directories for training and testing data**\n",
        "train_dir = \"/content/drive/MyDrive/Indian-Birds-Detection/BirdsDoc-Dataset/preprocessed-dataset/train\"\n",
        "test_dir = \"/content/drive/MyDrive/Indian-Birds-Detection/BirdsDoc-Dataset/preprocessed-dataset/test\"\n",
        "\n",
        "# **Define transformations for image preprocessing**\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# **Load full datasets**\n",
        "full_train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms[\"train\"])\n",
        "full_test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms[\"test\"])\n",
        "\n",
        "# **Function to select a limited number of images per class**\n",
        "def get_limited_dataset(full_dataset, max_images_per_class):\n",
        "    class_to_indices = {class_idx: [] for class_idx in range(len(full_dataset.classes))}\n",
        "\n",
        "    # Collect indices of images belonging to each class\n",
        "    for idx, (_, label) in enumerate(full_dataset.samples):\n",
        "        class_to_indices[label].append(idx)\n",
        "\n",
        "    # Select max_images_per_class randomly from each class\n",
        "    selected_indices = []\n",
        "    for indices in class_to_indices.values():\n",
        "        selected_indices.extend(random.sample(indices, min(len(indices), max_images_per_class)))\n",
        "\n",
        "    return Subset(full_dataset, selected_indices)\n",
        "\n",
        "# **Create limited datasets (Set max_images_per_class to 800 for better accuracy)**\n",
        "train_dataset = get_limited_dataset(full_train_dataset, max_images_per_class)\n",
        "test_dataset = get_limited_dataset(full_test_dataset, max_images_per_class)\n",
        "\n",
        "# **Load Data into DataLoader**\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# **Load a pretrained EfficientNet and modify for our dataset**\n",
        "model = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")  # Faster and better than ResNet50\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, num_classes)  # Modify for 25 bird classes\n",
        "\n",
        "# **Define loss function and optimizer**\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# **Move model to GPU if available**\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# **Ensure 'models' folder exists**\n",
        "model_save_dir = \"/content/drive/MyDrive/Indian-Birds-Detection/BirdsDoc-Dataset/models\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# **Load checkpoint if available**\n",
        "def load_checkpoint():\n",
        "    checkpoint_path = os.path.join(model_save_dir, \"checkpoint.pth\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(\"‚úÖ Checkpoint found! Resuming training...\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch_start = checkpoint['epoch']\n",
        "        return epoch_start\n",
        "    else:\n",
        "        print(\"üöÄ No checkpoint found. Starting from scratch!\")\n",
        "        return 0\n",
        "\n",
        "# **Save checkpoint**\n",
        "def save_checkpoint(epoch):\n",
        "    checkpoint_path = os.path.join(model_save_dir, \"checkpoint.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"üíæ Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# **Training loop with progress tracking**\n",
        "def train_model():\n",
        "    start_epoch = load_checkpoint()\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        num_batches = len(train_loader)\n",
        "\n",
        "        print(f\"\\nüî• Starting epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            progress = (batch_idx + 1) / num_batches * 100\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{num_batches} - Loss: {running_loss / (batch_idx + 1):.4f}, Progress: {progress:.2f}%\")\n",
        "\n",
        "        save_checkpoint(epoch + 1)\n",
        "\n",
        "    print(\"‚úÖ Training complete!\")\n",
        "\n",
        "# **Evaluation loop**\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"üéØ Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# **Run training and evaluation**\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n",
        "    evaluate_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MizVJB8RZOK5",
        "outputId": "d470911e-53fd-440d-f73d-352611416f9b"
      },
      "outputs": [],
      "source": [
        "%pip install streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### this is simply app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud1qRZGxZQ9u",
        "outputId": "b679dd13-5f37-4339-b5bc-069be37a26c1"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Set up the page configuration\n",
        "st.set_page_config(page_title=\"Indian Common Bird Detector\", layout=\"wide\")\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "@st.cache_resource\n",
        "def load_model(model_path=\"/content/drive/MyDrive/Indian-Birds-Detection/BirdsDoc-Dataset/models/checkpoint.pth\", num_classes=25):\n",
        "    model = models.efficientnet_b0(pretrained=False)  # ‚úÖ Fixed: Using EfficientNet-B0\n",
        "    num_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = torch.nn.Linear(num_features, num_classes)  # ‚úÖ Fixed: Match train.py\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        print(f\"‚úÖ Loaded model checkpoint from: {model_path}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No checkpoint found, loading model from scratch.\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Helper function to predict\n",
        "def predict(image, model, class_names):\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        confidence = torch.nn.functional.softmax(outputs, dim=1)[0][predicted].item()\n",
        "    return class_names[predicted.item()], confidence\n",
        "\n",
        "# Load class names\n",
        "@st.cache_resource\n",
        "def load_class_names():\n",
        "    return ['Asian Green Bee Eater', 'Brown Headed Barbet', 'Cattle Egret', 'Common Kingfisher', 'Common Myna',\n",
        "            'Common Rosefinch', 'Common Tailorbird', 'Coppersmith Barbet', 'Forest Wagtail', 'Gray Wagtail',\n",
        "            'Hoopoe', 'House Crow', 'Indian Grey Hornbill', 'Indian Peacock', 'Indian Pitta', 'Indian Roller',\n",
        "            'Jungle Babbler', 'Northern Lapwing', 'Red Wattled Lapwing', 'Ruddy Shelduck', 'Rufous Treepie',\n",
        "            'Sarus Crane', 'White Breasted Kingfisher', 'White Breasted Waterhen', 'White Wagtail']\n",
        "\n",
        "class_names = load_class_names()\n",
        "\n",
        "# UI Layout\n",
        "st.title(\"ü¶ú Indian Birds Detector\")\n",
        "st.markdown(\"Upload an image to detect **Indian bird species** using a deep learning model.\")\n",
        "\n",
        "# Upload and display image\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\", \"webp\"])\n",
        "if uploaded_file:\n",
        "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "\n",
        "    # Resize image before displaying to prevent taking full height\n",
        "    image_resized = image.resize((300, 300))  # ‚úÖ Set max size\n",
        "\n",
        "    # Use columns to center the image and reduce space usage\n",
        "    col1, col2, col3 = st.columns([1, 2, 1])  # Centering layout\n",
        "    with col2:\n",
        "        st.image(image_resized, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "    # Predict button\n",
        "    if st.button(\"Predict\"):\n",
        "        class_name, confidence = predict(image, model, class_names)\n",
        "        st.subheader(f\"Prediction: **{class_name}**\")\n",
        "        st.write(f\"Confidence: **{confidence * 100:.2f}%**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxYz1tjeZV37",
        "outputId": "6ccf31ca-0127-47b6-c9d2-1fbeaccd3d70"
      },
      "outputs": [],
      "source": [
        "%pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1yUXZGIZXcA",
        "outputId": "354af28d-672f-4937-9918-25b43a5caf4e"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken your_ngrok_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS-9yommZaLb",
        "outputId": "ed0c8e02-bb89-4478-8389-d0a4cb52c64b"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Start ngrok tunnel to the Streamlit app (default port is 8501)\n",
        "public_url = ngrok.connect('8501')\n",
        "print(f\"Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Run Streamlit app in the background\n",
        "!streamlit run app.py &\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
